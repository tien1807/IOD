{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QaGbHIhQLFYq"
   },
   "source": [
    "<div>\n",
    "<img src=https://www.institutedata.com/wp-content/uploads/2019/10/iod_h_tp_primary_c.svg width=\"300\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMx6dqz7LFYv"
   },
   "source": [
    "# Lab 3.2.3\n",
    "# *The Google BigQuery UI and API*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SKGiPVSwLFYx"
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKMV45iQLFY2"
   },
   "source": [
    "The Google BigQuery UI provides access to Google's extensive collection of public data sets via an SQL-based query engine.\n",
    "\n",
    "The BigQuery API provides programmatic access to the data sets.\n",
    "\n",
    "We can use the UI to discover interesting data before writing Python code to access it. Then we can reproduce it in an API request so as to aggregate large amounts of data on Google's infrastructure before pulling the results into our application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q3JonnfULFY6"
   },
   "source": [
    "## BigQuery Web UI\n",
    "\n",
    "Work through the Quickstart at https://cloud.google.com/bigquery/docs/quickstarts/quickstart-web-ui.\n",
    "\n",
    "You will need to set up a Google Cloud Platform account if you don't already have one. (This should not cost anything during the trial period unless you perform a large amount of querying. Afterwards, costs are based on actual resource usage, but most offerings have a free tier.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QM5QOIBpLFY9"
   },
   "source": [
    "## BigQuery API\n",
    "\n",
    "You should already have the Google Cloud Client Library for Python installed (https://cloud.google.com/python/setup).\n",
    "\n",
    "- Open Google Cloud Console (https://console.cloud.google.com/home/) and select to create a project.\n",
    "\n",
    "- Under \"Getting Started\", select \"Enable APIs and get credentials such as keys\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Ni9g5D7LFY_"
   },
   "source": [
    "- In the API table, make sure the BigQuery API is enabled. Page back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYjui9nrLFZC"
   },
   "source": [
    "### Authentication\n",
    "\n",
    "Go to https://cloud.google.com/iam/docs/creating-managing-service-account-keys and follow the instructions under \"Create a service account key\" to create a service account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OYryTo8SLFZE"
   },
   "source": [
    "- Fill out the form, giving the account an appropriate name, and choose \"Project Owner\" for Account Type.\n",
    "\n",
    "- Click \"Create\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUx6uvpALFZF"
   },
   "source": [
    "- The keys will get saved to your computer.\n",
    "\n",
    "- Note the location and copy the file path (of the json file) to somewhere safe, for future reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DnYr6eJ-LFZH"
   },
   "source": [
    "- See here for more information:\n",
    "\n",
    "https://cloud.google.com/iam/docs/understanding-service-accounts?&_ga=2.173177830.-495703703.1532572448#managing_service_account_keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5pvHgwVcLFZJ"
   },
   "source": [
    "### Using the Python API\n",
    "\n",
    "Google provides Python libraries for wrapping the Google APIs.\n",
    "\n",
    "__(Installing the \"google-cloud-storage\" and \"google-cloud-bigquery\" libraries should cover all the dependencies for this lab.)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "-CFdsY-N6zeO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-2.18.1-py2.py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting google-auth<3.0dev,>=2.26.1 (from google-cloud-storage)\n",
      "  Downloading google_auth-2.33.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-api-core<3.0.0dev,>=2.15.0 (from google-cloud-storage)\n",
      "  Downloading google_api_core-2.19.1-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-cloud-core<3.0dev,>=2.3.0 (from google-cloud-storage)\n",
      "  Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting google-resumable-media>=2.6.0 (from google-cloud-storage)\n",
      "  Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in c:\\users\\tien\\anaconda3\\envs\\iod\\lib\\site-packages (from google-cloud-storage) (2.31.0)\n",
      "Collecting google-crc32c<2.0dev,>=1.0 (from google-cloud-storage)\n",
      "  Downloading google_crc32c-1.5.0-cp310-cp310-win_amd64.whl.metadata (2.3 kB)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage)\n",
      "  Downloading googleapis_common_protos-1.63.2-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage)\n",
      "  Downloading protobuf-5.27.3-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-api-core<3.0.0dev,>=2.15.0->google-cloud-storage)\n",
      "  Downloading proto_plus-1.24.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3.0dev,>=2.26.1->google-cloud-storage)\n",
      "  Downloading cachetools-5.4.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3.0dev,>=2.26.1->google-cloud-storage)\n",
      "  Downloading pyasn1_modules-0.4.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3.0dev,>=2.26.1->google-cloud-storage)\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tien\\anaconda3\\envs\\iod\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tien\\anaconda3\\envs\\iod\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tien\\anaconda3\\envs\\iod\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tien\\anaconda3\\envs\\iod\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2024.2.2)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=2.26.1->google-cloud-storage)\n",
      "  Downloading pyasn1-0.6.0-py2.py3-none-any.whl.metadata (8.3 kB)\n",
      "Downloading google_cloud_storage-2.18.1-py2.py3-none-any.whl (130 kB)\n",
      "   ---------------------------------------- 0.0/130.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 130.5/130.5 kB 3.9 MB/s eta 0:00:00\n",
      "Downloading google_api_core-2.19.1-py3-none-any.whl (139 kB)\n",
      "   ---------------------------------------- 0.0/139.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 139.4/139.4 kB 4.2 MB/s eta 0:00:00\n",
      "Downloading google_auth-2.33.0-py2.py3-none-any.whl (200 kB)\n",
      "   ---------------------------------------- 0.0/200.5 kB ? eta -:--:--\n",
      "   -------------------------------------- - 194.6/200.5 kB 5.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 200.5/200.5 kB 6.1 MB/s eta 0:00:00\n",
      "Downloading google_cloud_core-2.4.1-py2.py3-none-any.whl (29 kB)\n",
      "Downloading google_crc32c-1.5.0-cp310-cp310-win_amd64.whl (27 kB)\n",
      "Downloading google_resumable_media-2.7.2-py2.py3-none-any.whl (81 kB)\n",
      "   ---------------------------------------- 0.0/81.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 81.3/81.3 kB 4.4 MB/s eta 0:00:00\n",
      "Downloading cachetools-5.4.0-py3-none-any.whl (9.5 kB)\n",
      "Downloading googleapis_common_protos-1.63.2-py2.py3-none-any.whl (220 kB)\n",
      "   ---------------------------------------- 0.0/220.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 220.0/220.0 kB 6.6 MB/s eta 0:00:00\n",
      "Downloading proto_plus-1.24.0-py3-none-any.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.1/50.1 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading protobuf-5.27.3-cp310-abi3-win_amd64.whl (426 kB)\n",
      "   ---------------------------------------- 0.0/426.9 kB ? eta -:--:--\n",
      "   ------------------------- -------------- 276.5/426.9 kB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 426.9/426.9 kB 5.3 MB/s eta 0:00:00\n",
      "Downloading pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "   ---------------------------------------- 0.0/181.2 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 163.8/181.2 kB 5.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 181.2/181.2 kB 5.5 MB/s eta 0:00:00\n",
      "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "   ---------------------------------------- 0.0/85.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 85.3/85.3 kB 5.0 MB/s eta 0:00:00\n",
      "Installing collected packages: pyasn1, protobuf, google-crc32c, cachetools, rsa, pyasn1-modules, proto-plus, googleapis-common-protos, google-resumable-media, google-auth, google-api-core, google-cloud-core, google-cloud-storage\n",
      "Successfully installed cachetools-5.4.0 google-api-core-2.19.1 google-auth-2.33.0 google-cloud-core-2.4.1 google-cloud-storage-2.18.1 google-crc32c-1.5.0 google-resumable-media-2.7.2 googleapis-common-protos-1.63.2 proto-plus-1.24.0 protobuf-5.27.3 pyasn1-0.6.0 pyasn1-modules-0.4.0 rsa-4.9\n"
     ]
    }
   ],
   "source": [
    "!pip install google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Exi8RHoh65Ic"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-bigquery\n",
      "  Downloading google_cloud_bigquery-3.25.0-py2.py3-none-any.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1 in c:\\users\\tien\\anaconda3\\envs\\iod\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (2.19.1)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in c:\\users\\tien\\anaconda3\\envs\\iod\\lib\\site-packages (from google-cloud-bigquery) (2.33.0)\n",
      "Requirement already satisfied: google-cloud-core<3.0.0dev,>=1.6.0 in c:\\users\\tien\\anaconda3\\envs\\iod\\lib\\site-packages (from google-cloud-bigquery) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media<3.0dev,>=0.6.0 in c:\\users\\tien\\anaconda3\\envs\\iod\\lib\\site-packages (from google-cloud-bigquery) (2.7.2)\n",
      "Requirement already satisfied: packaging>=20.0.0 in c:\\users\\tien\\anaconda3\\envs\\iod\\lib\\site-packages (from google-cloud-bigquery) (23.2)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in c:\\users\\tien\\anaconda3\\envs\\iod\\lib\\site-packages (from google-cloud-bigquery) (2.8.2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.21.0 in c:\\users\\tien\\anaconda3\\envs\\iod\\lib\\site-packages (from google-cloud-bigquery) (2.31.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\tien\\anaconda3\\envs\\iod\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (1.63.2)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in c:\\users\\tien\\anaconda3\\envs\\iod\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (5.27.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\tien\\anaconda3\\envs\\iod\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery) (1.24.0)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery)\n",
      "  Downloading grpcio-1.65.4-cp310-cp310-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-cloud-bigquery)\n",
      "  Downloading grpcio_status-1.65.4-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\tien\\anaconda3\\envs\\iod\\lib\\site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (5.4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\tien\\anaconda3\\envs\\iod\\lib\\site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (0.4.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\tien\\anaconda3\\envs\\iod\\lib\\site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (4.9)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in c:\\users\\tien\\anaconda3\\envs\\iod\\lib\\site-packages (from google-resumable-media<3.0dev,>=0.6.0->google-cloud-bigquery) (1.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\tien\\anaconda3\\envs\\iod\\lib\\site-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\tien\\anaconda3\\envs\\iod\\lib\\site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tien\\anaconda3\\envs\\iod\\lib\\site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tien\\anaconda3\\envs\\iod\\lib\\site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tien\\anaconda3\\envs\\iod\\lib\\site-packages (from requests<3.0.0dev,>=2.21.0->google-cloud-bigquery) (2024.2.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in c:\\users\\tien\\anaconda3\\envs\\iod\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-bigquery) (0.6.0)\n",
      "Downloading google_cloud_bigquery-3.25.0-py2.py3-none-any.whl (239 kB)\n",
      "   ---------------------------------------- 0.0/239.0 kB ? eta -:--:--\n",
      "   ----- ---------------------------------- 30.7/239.0 kB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 239.0/239.0 kB 3.7 MB/s eta 0:00:00\n",
      "Downloading grpcio-1.65.4-cp310-cp310-win_amd64.whl (4.1 MB)\n",
      "   ---------------------------------------- 0.0/4.1 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.3/4.1 MB 8.3 MB/s eta 0:00:01\n",
      "   ------ --------------------------------- 0.7/4.1 MB 8.7 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.1/4.1 MB 8.7 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 1.5/4.1 MB 8.6 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 2.0/4.1 MB 8.9 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 2.3/4.1 MB 8.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 2.6/4.1 MB 8.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 2.8/4.1 MB 8.5 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 2.9/4.1 MB 7.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 3.5/4.1 MB 7.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.8/4.1 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  4.1/4.1 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.1/4.1 MB 7.3 MB/s eta 0:00:00\n",
      "Downloading grpcio_status-1.65.4-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: grpcio, grpcio-status, google-cloud-bigquery\n",
      "Successfully installed google-cloud-bigquery-3.25.0 grpcio-1.65.4 grpcio-status-1.65.4\n"
     ]
    }
   ],
   "source": [
    "!pip install google-cloud-bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "0o5eABcXLFZL"
   },
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "from google.cloud.bigquery import Dataset\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHZQSxxqLFZR"
   },
   "source": [
    "Invoke a method of the `.Client` object that takes the path to your key files as a string argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "8nTBh0TvLFZR"
   },
   "outputs": [],
   "source": [
    "key_path = 'C:/Users/allen/Documents/My Important Stuff/Key Pairs/myreallybigquery-3e3aa70941f0.json' # put the path to your json key file here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0XKIHVy8LFZT"
   },
   "source": [
    "This should not throw an error if key retrieval / assignment worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gfqDJPU9LFZT"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/allen/Documents/My Important Stuff/Key Pairs/myreallybigquery-3e3aa70941f0.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m storage_client \u001b[38;5;241m=\u001b[39m \u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mClient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_service_account_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey_path\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\IOD\\lib\\site-packages\\google\\cloud\\client\\__init__.py:106\u001b[0m, in \u001b[0;36m_ClientFactoryMixin.from_service_account_json\u001b[1;34m(cls, json_credentials_path, *args, **kwargs)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_service_account_json\u001b[39m(\u001b[38;5;28mcls\u001b[39m, json_credentials_path, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     86\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Factory to retrieve JSON credentials while creating client.\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \n\u001b[0;32m     88\u001b[0m \u001b[38;5;124;03m    :type json_credentials_path: str\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;124;03m             and the credentials created by the factory.\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjson_credentials_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m json_fi:\n\u001b[0;32m    107\u001b[0m         credentials_info \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(json_fi)\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfrom_service_account_info(credentials_info, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/allen/Documents/My Important Stuff/Key Pairs/myreallybigquery-3e3aa70941f0.json'"
     ]
    }
   ],
   "source": [
    "storage_client = storage.Client.from_service_account_json(key_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1sntjgJLFZU"
   },
   "source": [
    "*Nb. The `storage` object was used in the above example, but there are other objects of interest that have polymorphic `Client` members that are used similarly, such as `bigquery`, which is used below.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zSA3wp7NLFZX"
   },
   "source": [
    "Next, execute this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljcqTlxXLFZX"
   },
   "outputs": [],
   "source": [
    "client = bigquery.Client.from_service_account_json(key_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kS1b8GXYLFZY"
   },
   "source": [
    "This client is associated with the default project (which was set or defaulted in the BigQuery UI):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LflN4IlzLFZY"
   },
   "outputs": [],
   "source": [
    "client.project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BXQNVF1XLFZb"
   },
   "source": [
    "A BigQuery project contains datasets. Datasets contain tables. To get at the data in a table we need to create a reference that covers this hierarchy; in the `bigquery` library this looks like `project.dataset.table`.  \n",
    "\n",
    "(Nb. Queries can be performed on projects and datasets, but most queries are performed on tables.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fOtLSyBdLFZb"
   },
   "source": [
    "To explore the public datasets we will start by reassigning our `client` variable using optional `project` parameter (set to `bigquery-public-data`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SQTQ24whLFZc"
   },
   "outputs": [],
   "source": [
    "#project = 'bigquery-public-data'\n",
    "client = bigquery.Client.from_service_account_json(key_path, project = 'bigquery-public-data')\n",
    "print(client.project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZonWy10xLFZe"
   },
   "source": [
    "Here is how to get a list of the datasets in the current project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQan0v1vLFZe"
   },
   "outputs": [],
   "source": [
    "datasets = list(client.list_datasets())\n",
    "print(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UoYS4suLFZg"
   },
   "source": [
    "That wasn't helpful. We need to go deeper into the object structure to get at something meaningful. Actually, the `dataset_id` member contains the name attribute of a `dataset` object; write some code to print that name for each member of the list that was created above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MBV3-c21LFZi"
   },
   "outputs": [],
   "source": [
    "#?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1VKuobUSLFZk"
   },
   "source": [
    "The google API objects in the `bigquery` library have their own overloads of the format() function that make them easier to read. Below is a function that exploits the `format` method of `project` and `dataset_id`, providing an easy way to list datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "obqd0X0vLFZl"
   },
   "outputs": [],
   "source": [
    "# function for listing datasets in a project:\n",
    "def printDatasetList(client):\n",
    "    project = client.project    #: only one project can be associated with a client instance\n",
    "    datasets = list(client.list_datasets())\n",
    "    if datasets:\n",
    "        print('Datasets in project {}:'.format(project))\n",
    "        for dataset in datasets:\n",
    "            print('\\t{}'.format(dataset.dataset_id))\n",
    "        found = True\n",
    "    else:\n",
    "        print('{} project does not contain any datasets.'.format(project))\n",
    "        found = False\n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TcS8DXenLFZn"
   },
   "outputs": [],
   "source": [
    "# list datasets in the default project:\n",
    "flag = printDatasetList(client)  #: assigning to `flag` suppresses printing the return value (normally `True`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81uawTW5LFZo"
   },
   "source": [
    "This list should correspond to what is shown here https://bigquery.cloud.google.com/publicdatasets under the **bigquery-public-data** item."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bEAfjVxOLFZo"
   },
   "source": [
    "Here is how to create a dataset reference object by assigning a project and a dataset name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZrSaLIkWLFZo"
   },
   "outputs": [],
   "source": [
    "dataset_id = 'samples'\n",
    "dataset_ref = client.dataset(dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5BlLbSA7LFZp"
   },
   "source": [
    "If our current project was something other than `bigquery-public-data`, we could still create this reference by specifying the project that contains the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJK5yM9BLFZq"
   },
   "outputs": [],
   "source": [
    "dataset_id = 'samples'\n",
    "dataset_ref = client.dataset(dataset_id, project = 'bigquery-public-data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6_fYqMyGLFZr"
   },
   "source": [
    "How can we get the path of the dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "otqzZohpLFZs"
   },
   "outputs": [],
   "source": [
    "#?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRzK-hTuLFZt"
   },
   "source": [
    "Explore more of this object's members:\n",
    "\n",
    "*(HINT: Jupyter Notebooks does not support code completion, but Spyder and other Python IDEs do. If you copy all the above code to a Python file within the IDE, you can type `dataset_ref.` in a new line, then hit the [Tab] key to see the available members for the object.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s9HM7PiMLFZt"
   },
   "outputs": [],
   "source": [
    "#?\n",
    "dataset_ref."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZB6yeerLFZu"
   },
   "source": [
    "Here is a function for listing the tables in a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_E5S-MhtLFZu"
   },
   "outputs": [],
   "source": [
    "# function for listing tables in a dataset:\n",
    "def printTableList(client, dataset_id):\n",
    "    project = client.project\n",
    "    dataset_ref = client.dataset(dataset_id, project = project)\n",
    "    tables = list(client.list_tables(dataset_ref))\n",
    "    if tables:\n",
    "        print('Tables in dataset {}:'.format(dataset_id))\n",
    "        for table in tables:\n",
    "            print('\\t{}'.format(table.table_id))\n",
    "        found = True\n",
    "    else:\n",
    "        print('{} dataset does not contain any tables.'.format(dataset_id))\n",
    "        found = False\n",
    "    return found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ksp2VbNcLFZv"
   },
   "source": [
    "Use this function to list the tables in the current dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F2KEisFzLFZv"
   },
   "outputs": [],
   "source": [
    "#?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XoZPwFx1LFZx"
   },
   "source": [
    "To create a reference to a table within the dataset, we use the `table_id` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rYXUHik0LFZx"
   },
   "outputs": [],
   "source": [
    "table_id = 'shakespeare'\n",
    "table_ref = dataset_ref.table(table_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3MZOaAuiLFZy"
   },
   "source": [
    "Check the name of the table that `table_ref` now points to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6B9AAg4tLFZz"
   },
   "outputs": [],
   "source": [
    "#?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOo7O5lVLFZ0"
   },
   "source": [
    "To access the data in the table itself, we use the `get_table()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rjnXTIoULFZ0"
   },
   "outputs": [],
   "source": [
    "table = client.get_table(table_ref)  # API Request"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ggy5pIrYLFZ2"
   },
   "source": [
    "NOTE: The contents of the table are not actually in our memory after this call! We are working with a Big Data platform, now, and we could easily end up pulling GBs or TBs of data by accident.\n",
    "\n",
    "To minimise data bandwidth, memory consumption, and processing time, Big Data platforms employ ***lazy evaluation***. This means that no computation or data transfer actually takes place until we *realise* (use) the data. Even if we execute subsequent code that performs calculations on the data, no data flow or computation actually occurs until we request output (e.g. by executing a print to stdout or writing to a file)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Sa9uV6tLFZ2"
   },
   "source": [
    "What kind of object is returned by `client.get_table`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cdmNEp6ZLFZ3"
   },
   "outputs": [],
   "source": [
    "#?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcaJMPCNLFZ4"
   },
   "source": [
    "How can we view the design of the table (column names and types? The name of the object attribute we need is the same term we learned in the module on databases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gt4FQZ88LFZ4"
   },
   "outputs": [],
   "source": [
    "#?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FfaKrQRSLFZ6"
   },
   "source": [
    "Again, this is messy. If we wanted to refer to the column names and types in code, we might use something like this (which we could then parse into a dict):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8187Fu57LFZ6"
   },
   "outputs": [],
   "source": [
    "result = [\"{0} {1}\".format(schema.name,schema.field_type) for schema in table.schema]\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZQz7xhfwLFZ7"
   },
   "source": [
    "But if we just want to print them, here is another neat function for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_LqvAXo3LFZ7"
   },
   "outputs": [],
   "source": [
    "# function to print a table schema:\n",
    "def printTableSchema(aTable):\n",
    "    schemas = list(aTable.schema)\n",
    "    if schemas:\n",
    "        print('Table schema for {}:'.format(aTable.table_id))\n",
    "        for aSchema in schemas:\n",
    "            print('\\t{0} {1}'.format(aSchema.name, aSchema.field_type))\n",
    "        found = True\n",
    "    else:\n",
    "        found = False\n",
    "    return found"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SROTDIm8LFZ8"
   },
   "source": [
    "Use this function to print the table schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DpmWRmXmLFZ9"
   },
   "outputs": [],
   "source": [
    "#?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1YugUUhLFZ-"
   },
   "source": [
    "Now that we know what the columns are, we can write queries. Actually, we construct a query job by assigning an SQL statement to a method of the `client` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gq7cGALfLFZ-"
   },
   "outputs": [],
   "source": [
    "sql = \"SELECT COUNT(1) FROM bigquery-public-data.samples.shakespeare\"\n",
    "query_job = client.query(sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DJVssU3mLFZ_"
   },
   "source": [
    "Why does this throw an error?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4feq3JaqLFZ_"
   },
   "source": [
    "ANSWER:\n",
    "\n",
    "_____________________________________________________\n",
    "\n",
    "So, what can we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hCVH3AxrLFaA"
   },
   "outputs": [],
   "source": [
    "#?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTfuF8J0LFaA"
   },
   "source": [
    "If that worked, show what query_job is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZmiGhS0ALFaA"
   },
   "outputs": [],
   "source": [
    "#?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Src6_5auLFaB"
   },
   "source": [
    "Once again, due to lazy execution, no actual execution occurs until we request output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FCz4Yw9jLFaB"
   },
   "outputs": [],
   "source": [
    "for row in query_job:  # API request - fetches results\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CQG3a1oALFaC"
   },
   "source": [
    "And, again, we need to manipulate this to make it neat. Each member of the rowset is a list and we only want to extract the value, which is in the first member:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qcq2F5vzLFaC"
   },
   "outputs": [],
   "source": [
    "print(row[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cPqWb1_YLFaD"
   },
   "source": [
    "So, we now know that this table has 164,656 rows. (We would not want to print it!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6idZY0-3LFaD"
   },
   "source": [
    "A better coding practice is to write SQL statements that assign names (aliases) to derived values, so we don't forget what the resulting rowset contains. Rewrite the above SQL statement so that the value returned is aliased a \"num_rows\", and assign the QueryJob as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NeHVJAm5LFaD"
   },
   "outputs": [],
   "source": [
    "#?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUFlZ66lLFaE"
   },
   "source": [
    "Now we could use Python's `assert` statement to build a test into the first code block that operates on the rowset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6MHaNZh0LFaE"
   },
   "outputs": [],
   "source": [
    "for row in query_job:  # API request - fetches results\n",
    "    # Row values can be accessed by field name or index:\n",
    "    assert row[0] == row.num_rows == row['num_rows']  #: for debugging bad sql\n",
    "    print(row.num_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fP4GpU02LFaF"
   },
   "source": [
    "The above code checks that the name attribute of the value in `row[0]` is what we expected (i.e. \"num_rows\"). Also, it shows that we can refer to a field in a row by its object member `num_rows` or by using the same notation we use for Python dictionaries, `['num_rows']`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y83gunLZLFaF"
   },
   "source": [
    "Write, execute, and print the results of a query that fetches 10 rows from the table, each containing the \"word\", \"word_count\", and \"corpus\" fields:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xhvBwHzpLFaG"
   },
   "outputs": [],
   "source": [
    "#?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OlGpm9dmLFaH"
   },
   "source": [
    "(NOTE: Using `assert` religiously is good practice and will make debugging easier, but is probably overkill for non-production code.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "md6tWVvHLFaH"
   },
   "source": [
    "Whenever you catch yourself writing a swag of code to do something that seems rudimentary or low-level, there is a very good chance that you don't need to. A much easier way to handle the above requirement is to use the `to_dataframe` method of the QueryJob object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "19bjagBHLFaH"
   },
   "outputs": [],
   "source": [
    "df = query_job.to_dataframe()\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qc7vdIfALFaJ"
   },
   "source": [
    "Although the above doesn't use `assert` (which you might still want to include in some test code), you will be able to tell at a glance if something is wrong with the contents of the DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DsWSuO95LFaJ"
   },
   "source": [
    "#### Final Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4mKynMuNLFaJ"
   },
   "source": [
    "1. Here is a readable way to code long SQL statements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8nm7i1-rLFaJ"
   },
   "outputs": [],
   "source": [
    "sql = \"\"\"\n",
    "    SELECT word, word_count, corpus\n",
    "    FROM bigquery-public-data.samples.shakespeare\n",
    "    LIMIT 10\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1j2w1aovLFaK"
   },
   "source": [
    "2. If you had an application that needed to modify the tables or datasets in the `bigquery-public-data` is project, you could copy them to our own project, where you would have the permissions to do as you please with the data (subject to Google's terms of use)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOAZyGTcLFaL"
   },
   "source": [
    "3. We aren't limited to the datasets that are already in BigQuery. We can upload tables from our computer, and we can pull data in from other online sources. We will cover these tasks in another module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PJmK2n7vLFaL"
   },
   "source": [
    "#### Next Steps\n",
    "\n",
    "If you wish to pick up a few more skills you can go to https://cloud.google.com/bigquery/create-simple-app-api. (Note that we have already been through the preliminaries, so you can start at \"Download the sample code\".)\n",
    "\n",
    "Alternatively, you can take a deeper dive into the API here: https://googlecloudplatform.github.io/google-cloud-python/latest/bigquery/usage.html."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFNsekmxLFaM"
   },
   "source": [
    "## - END -"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBAJLLeXLHs8"
   },
   "source": [
    ">\n",
    ">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1AVU7q-LIuO"
   },
   "source": [
    ">\n",
    ">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fpehj_5ELJk9"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "> > > > > > > > > Â© 2024 Institute of Data\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "PJmK2n7vLFaL"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
